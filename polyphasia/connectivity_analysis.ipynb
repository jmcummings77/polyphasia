{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "from networkx.algorithms.traversal import bfs_tree\n",
    "\n",
    "\"\"\"\n",
    "Load data from file\n",
    "\"\"\"\n",
    "\n",
    "# set up some paths for ins and outs\n",
    "base_directory = Path.cwd().parent.absolute()\n",
    "source_data_path = base_directory / \"data\" / \"raw\" / \"etymwn.tsv\"\n",
    "processed_directory = base_directory / \"data\" / \"processed\"\n",
    "output_graph_yaml_file = processed_directory / \"graph_output.yaml\"\n",
    "output_graph_dot_file = processed_directory / \"graph_output.dot\"\n",
    "output_graph_png = processed_directory / \"graph_output.png\"\n",
    "\n",
    "# load from CSV\n",
    "cleaned_df = pd.read_csv(\n",
    "    source_data_path, sep=\"\\t\", names=[\"source_node\", \"edge_type\", \"target_node\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Cleaning notes\n",
    "\n",
    "## Relationship types found in source data\n",
    "\n",
    "Relationships are recorded bidirectionally. So a root word will link to its derivatives, and each derivative will link back to the root. To simplify the graph,\n",
    "I will drop edges that point from derivatives to roots, since that information is already encoded in the root-to-leaf edge and networkx can handle bidirectional\n",
    "traversal without requiring multiple edges to link the same pair of nodes.\n",
    "\n",
    "Below are the types of relationships extracted from the data, with comments indicating their directionality.\n",
    "\n",
    "\n",
    "<- A is the source of B\n",
    "\n",
    "-> B is the source of A\n",
    "\n",
    "\n",
    "- \"rel:etymology\"               ->\n",
    "- \"rel:etymological_origin_of\"  <-\n",
    "- \"rel:is_derived_from\"         ->\n",
    "- \"rel:has_derived_form\"        <-\n",
    "- \"rel:etymologically_related\"  <->\n",
    "- \"rel:variant:orthography\"     <->\n",
    "\n",
    "source data also includes a handful of malformed values, which should be dropped or replaced \n",
    "- \"rel:etymologically\" -> \"rel:etymologically_related\"\n",
    "- \"rel:derived\" -> \"rel:is_derived_from\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Clean data\n",
    "\"\"\"\n",
    "\n",
    "# filter out bidirectional relationships and select one directionality to normalize the graph\n",
    "# I would normally clean to fix a handful of malformed tags as below but we are dropping those edge types anyway\n",
    "# so instead we will stick to the edge types that point from root words to derived words\n",
    "root_first_rel_types = [\"rel:etymological_origin_of\", \"rel:has_derived_form\"]\n",
    "cleaned_df = cleaned_df.loc[(cleaned_df[\"edge_type\"].isin(root_first_rel_types))]\n",
    "cleaned_df[[\"source_language\", \"source_word\"]] = cleaned_df.source_node.str.split(\n",
    "    \": \", expand=True\n",
    ")\n",
    "\n",
    "# there are a handful of nodes that include strange characters or a :Category: tag that introduces a third\n",
    "# column for no reason. This data is uninteresting so we can just ignore it and no include it in the graph when we construct it\n",
    "cleaned_df[[\"target_language\", \"target_word\", \"crud\"]] =  cleaned_df.target_node.str.split(\": \", expand=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{'ave', 'frk', 'xno', 'hif', 'sot', 'sun', 'cic', 'jbo', 'umu', 'twf', 'ara', 'tam', 'adt', 'ast', 'mar', 'lij', 'grn', 'kri', 'auc', 'chc', 'kan', 'heb', 'slv', 'chr', 'kjh', 'urd', 'ukr', 'tha', 'szl', 'lim', 'mod', 'p_gmw', 'frp', 'lkt', 'gla', 'tew', 'tat', 'xon', 'rom', 'hye', 'chu', 'tuk', 'sco', 'slk', 'syc', 'haw', 'efi', 'p_sla', 'nor', 'aaq', 'dan', 'ppl', 'wit', 'akk', 'xng', 'txb', 'lmo', 'zko', 'gsw', 'tsn', 'aii', 'hsb', 'nap', 'arw', 'vls', 'byn', 'san', 'kur', 'swa', 'nob', 'pro', 'rap', 'tgl', 'ota', 'gwi', 'cym', 'cha', 'sme', 'nld', 'wln', 'nan', 'dsb', 'zai', 'gmy', 'ksd', 'tnq', 'tir', 'ron', 'sms', 'ike', 'mri', 'dtd', 'eus', 'som', 'odt', 'oji', 'fry', 'rme', 'stg', 'ben', 'sqi', 'ryu', 'sux', 'sei', 'min', 'epo', 'quc', 'nys', 'see', 'xho', 'mxi', 'prg', 'ksh', 'xaa', 'lua', 'frm', 'mya', 'glv', 'cor', 'osp', 'mal', 'nah', 'kaw', 'ccc', 'hak', 'sat', 'bod', 'ase', 'ale', 'ltc', 'fro', 'gez', 'por', 'mic', 'ood', 'rue', 'abs', 'kmb', 'mfr', 'vai', 'oge', 'gil', 'pml', 'hit', 'ltz', 'xtg', 'mnk', 'ton', 'wrh', 'lav', 'obt', 'nci', 'ang', 'pal', 'fur', 'aym', 'tah', 'xmb', 'gle', 'lng', 'crh', 'que', 'kaz', 'ell', 'oss', 'kzj', 'ofs', 'arg', 'cho', 'cat', 'fin', 'ina', 'fij', 'bis', 'deu', 'mlg', 'krl', 'ipk', 'bak', 'osx', 'xpr', 'hin', 'enm', 'nep', 'bdy', 'glg', 'yxg', 'bft', 'guj', 'iku', 'kir', 'xcl', 'pcd', 'arn', 'lad', 'lld', 'rmq', 'ndo', 'kin', 'kon', 'hun', 'rmf', 'tet', 'lug', 'khm', 'uzb', 'arz', 'lin', 'pan', 'che', 'swe', 'pol', 'tcs', 'vep', 'hat', 'est', 'enn', 'amj', 'ewe', 'tgk', 'pap', 'div', 'p_gem', 'scn', 'roh', 'cop', 'yid', 'dum', 'mbc', 'fao', 'srn', 'mav', 'aze', 'axm', 'evn', 'apw', 'gml', 'lat', 'ett', 'umb', 'nov', 'vma', 'srd', 'mon', 'ces', 'ayl', 'wam', 'csb', 'rus', 'kor', 'arq', 'afr', 'ruo', 'mah', 'fra', 'ess', 'hur', 'twi', 'orc', 'nds', 'zku', 'nno', 'srs', 'grc', 'ava', 'bel', 'luo', 'eng', 'ori', 'kld', 'mas', 'ary', 'pjt', 'bar', 'fon', 'sin', 'spa', 'egy', 'idb', 'xce', 'pli', 'cre', 'moe', 'ulk', 'mnc', 'pdt', 'isl', 'mlt', 'oco', 'xnt', 'naq', 'yor', 'kok', 'p_ine', 'rhg', 'lzh', 'rop', 'rup', 'lou', 'jam', 'mak', 'sth', 'tiv', 'pox', 'gmh', 'bua', 'stq', 'otk', 'vie', 'mwl', 'wlm', 'abe', 'akz', 'yua', 'tpi', 'peo', 'non', 'gul', 'kju', 'sga', 'bre', 'zsm', 'fas', 'okm', 'yur', 'obr', 'smo', 'pau', 'moh', 'alq', 'ceb', 'tpw', 'vec', 'hop', 'cmn', 'lut', 'xbm', 'shh', 'vol', 'kal', 'yol', 'pus', 'ikt', 'mkd', 'goh', 'wol', 'grv', 'kum', 'kat', 'ind', 'msa', 'ido', 'jpn', 'amh', 'kbd', 'lao', 'uig', 'orv', 'pim', 'qwc', 'zul', 'del', 'gae', 'wym', 'hbs', 'sna', 'mga', 'nav', 'nay', 'frc', 'jav', 'yue', 'hil', 'kky', 'phn', 'pis', 'tcy', 'tur', 'gug', 'hau', 'ain', 'emn', 'bul', 'owl', 'oci', 'ita', 'tel', 'got', 'inz', 'chn', 'dep', 'lit', 'myv'}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Get list of unique languages in data set.\n",
    "\n",
    "Not super necessary but helpful for understanding the likely subgraph structure. I would guess that the individual\n",
    "languages will be highly connected/clustered. I suspect the boundaries will blur a bit around the proto- and ancient languages, particularly for languages with\n",
    "many ancestors in the data set, e.g., Latin\n",
    "\"\"\"\n",
    "\n",
    "unique_languages = set(cleaned_df[\"source_language\"].unique()).union(set(cleaned_df[\"target_language\"].unique()))\n",
    "print(sorted(unique_languages))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Name: \nType: DiGraph\nNumber of nodes: 2743118\nNumber of edges: 2692096\nAverage in degree:   0.9814\nAverage out degree:   0.9814\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Construct networkx graph\n",
    "\"\"\"\n",
    "\n",
    "# start with directed so we can preserve directionality data, retaining the option to convert to undirected later to use networkx undirected algorithms\n",
    "graph = nx.from_pandas_edgelist(\n",
    "    cleaned_df,\n",
    "    edge_attr=[\n",
    "        \"edge_type\",\n",
    "        \"source_language\",\n",
    "        \"source_word\",\n",
    "        \"target_language\",\n",
    "        \"target_word\",\n",
    "    ],\n",
    "    source=\"source_node\",\n",
    "    target=\"target_node\",\n",
    "    create_using=nx.DiGraph,\n",
    ")\n",
    "print(nx.info(graph))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# What next?\n",
    "\n",
    "Now that we've loaded the data into a graph, it's time to do some actual analysis. But to do so, we need to define the problem more clearly. Otherwise, there is\n",
    "an intractable amount of data for many graph algorithms. Pruning to relevant subgraphs would be desirable as an initial post-processing step.\n",
    "\n",
    "I am primarily interested in English language entries. However, ~~many~~ all of the English words are derived from non-English words. It would be good to prune\n",
    "entries that are not etymological roots of English words.\n",
    "\n",
    "My initial instinct is to trim any descendant nodes of English words that are in other languages. Then, we can run BFS from each English node to gather its\n",
    "ancestors, knowing the descendant nodes have already been trimmed. This could accidentally exclude relevant data if there are derivation paths that jump from\n",
    "English to another language and then back, but that's an interesting question in and of itself and might be worth investigating as preliminary matter before\n",
    "pursuing this approach. \n",
    "\n",
    "However, I suspect the BFS approach may be extremely inefficient and that it will be necessary to reduce the size of the search space to something more\n",
    "tractable. \n",
    "\n",
    "So. What exactly are we looking for?\n",
    "\n",
    "- Nodes with the \"eng:\" prefix\n",
    "- Nodes that are direct and indirect ancestors of the English nodes\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DAG analysis\n",
    "\n",
    "Check if graph is directed acyclic\n",
    "\"\"\"\n",
    "\n",
    "from networkx.algorithms.dag import is_directed_acyclic_graph\n",
    "is_dag = is_directed_acyclic_graph(graph)\n",
    "print(is_dag)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "3305\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DAG analysis\n",
    "\n",
    "I expected that the graph would be acyclic, because logically it doesn't make sense of a word to be an ancestor of a word that is also the first word's ancestor.\n",
    "However, is_directed_acyclic_graph returned false, so I am going to check for cycles and see if there's something in the source data that can be cleaned up to\n",
    "enable DAG analysis\n",
    "\"\"\"\n",
    "from networkx.algorithms.cycles import simple_cycles\n",
    "cycles = list(simple_cycles(graph))\n",
    "cycle_count = len(cycles)\n",
    "print(cycle_count)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "True\n",
      "0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DAG analysis\n",
    "\n",
    "So... looks like all the cycles are self-connected single nodes. I'm guessing if I prune those it will be acyclic.\n",
    "\n",
    "Tried this, and still there were some loops left. they were in somewhat obscure languages so I decided to just drop them all.\n",
    "graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "\n",
    "\"\"\"\n",
    "from networkx.algorithms.dag import is_directed_acyclic_graph\n",
    "from networkx.algorithms.cycles import simple_cycles\n",
    "cycle_nodes = [node for cycle in list(simple_cycles(graph)) for node in cycle]\n",
    "graph.remove_nodes_from(cycle_nodes)\n",
    "is_dag = is_directed_acyclic_graph(graph)\n",
    "print(is_dag)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['grc: χάλιξ', 'lat: calx', 'lat: calceus', 'lat: calceō', 'lat: calceare', 'ita: calzare', 'ita: calza', 'ita: calzone', 'fra: caleçon', 'fra: caleçons', 'epo: kalsono', 'epo: kalsoneto']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DAG analysis\n",
    "\n",
    "Ok we have a DAG, time to see what kinds of fun patterns we can find...\n",
    "\n",
    "\"\"\"\n",
    "from networkx.algorithms.dag import dag_longest_path \n",
    "\n",
    "longest_path = dag_longest_path(graph)\n",
    "print(longest_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "342736\n",
      "96374\n",
      "Name: \nType: DiGraph\nNumber of nodes: 912653\nNumber of edges: 715717\nAverage in degree:   0.7842\nAverage out degree:   0.7842\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DAG analysis\n",
    "\n",
    "Create English-related subgraph\n",
    "\n",
    "Let's see if we can exploit the DAG properties to filter to chains with English words only\n",
    "\"\"\"\n",
    "roots = [node for node in nx.nodes(graph) if len(nx.ancestors(graph, node)) == 0]\n",
    "print(len(roots))\n",
    "english_nodes = [nx.descendants(graph, root) for root in roots if list(filter(lambda x: x[0:3] == \"eng\", nx.descendants(graph, root)))]\n",
    "print(len(english_nodes))\n",
    "english_graph = graph.subgraph([node for desc in english_nodes for node in desc])\n",
    "print(nx.info(english_graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[('eng: non-', 5748), ('eng: -ly', 5439), ('eng: un-', 4206), ('eng: -like', 2438), ('eng: -er', 2346), ('eng: -less', 1974), ('ita: -mente', 1666), ('eng: -able', 1428), ('eng: time', 1322), ('eng: anti-', 1293), ('eng: -y', 1227), ('eng: poly-', 1004), ('ita: -abile', 880), ('eng: -ic', 812), ('eng: -ed', 754), ('eng: multi-', 715), ('eng: over-', 710), ('spa: -mente', 681), ('fra: -ure', 665), ('eng: -ian', 625), ('eng: -ish', 588), ('eng: -ally', 520), ('eng: -ize', 500), ('eng: dog', 496), ('eng: disease', 493), ('deu: Stein', 485), ('eng: inter-', 461), ('fra: -er', 451), ('eng: hyper-', 445), ('eng: bird', 444), ('fra: -ique', 442), ('ita: -oso', 441), ('eng: be-', 428), ('eng: in-', 426), ('fra: -age', 424), ('ita: -tura', 419), ('eng: micro-', 401), ('eng: de-', 397), ('eng: post-', 395), ('eng: -an', 392), ('eng: -ity', 383), ('eng: -ing', 383), ('eng: -ship', 380), ('gle: -acht', 376), ('ita: -mento', 356), ('eng: Chinese', 346), ('eng: semi-', 327), ('eng: back', 326), ('ita: -ere', 326), ('eng: bread', 315), ('eng: a-', 305), ('eng: -istic', 298), ('eng: potato', 297), ('eng: -ment', 296), ('eng: seal', 284), ('eng: bi-', 281), ('glv: co-', 278), ('fra: -ité', 276), ('eng: green', 270), ('eng: leaf', 263), ('eng: snow', 256), ('eng: bio-', 254), ('ita: -ista', 251), ('eng: auto-', 250), ('epo: dek', 247), ('fra: in-', 247), ('eng: work', 246), ('eng: book', 242), ('eng: out-', 240), ('eng: sulfo-', 237), ('eng: -ferous', 233), ('eng: man', 231), ('eng: vine', 230), ('eng: head', 229), ('deu: -lich', 225), ('eng: pigeon', 224), ('eng: nano-', 222), ('fra: dé-', 219), ('eng: wax', 218), ('eng: neuro-', 217), ('eng: radio-', 217), ('eng: apple', 216), ('eng: wort', 212), ('ita: anti-', 212), ('lat: colligo', 212), ('eng: -wise', 208), ('eng: blue', 207), ('eng: -phyte', 205), ('eng: quasi-', 203), ('lat: praedico', 203), ('eng: ball', 201), ('eng: fish', 198), ('lat: colo', 197), ('lat: deligo', 195), ('fra: -iste', 195), ('eng: red', 194), ('lat: consterno', 194), ('lat: fundo', 193), ('lat: adgero', 192)]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "English graph analysis\n",
    "\n",
    "Inspect highest degree nodes\n",
    "\"\"\"\n",
    "nodes_by_degree = sorted(english_graph.degree, key=lambda x: x[1], reverse=True)\n",
    "print(nodes_by_degree[0:99])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/Source/polyphasia/.venv/lib/python3.9/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e939a94e300d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# convert to undirected so we can apply undirected algorithms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mundirected_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_undirected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mundirected_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/polyphasia/.venv/lib/python3.9/site-packages/networkx/classes/digraph.py\u001b[0m in \u001b[0;36mto_undirected\u001b[0;34m(self, reciprocal, as_view)\u001b[0m\n\u001b[1;32m   1182\u001b[0m         \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreciprocal\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             G.add_edges_from(\n",
      "\u001b[0;32m~/Source/polyphasia/.venv/lib/python3.9/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                     \u001b[0mnewdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Source/polyphasia/.venv/lib/python3.9/site-packages/networkx/classes/graph.py\u001b[0m in \u001b[0;36madd_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_node\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjlist_inner_dict_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                     \u001b[0mnewdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/PyCharm.app/Contents/helpers/pydev/_pydevd_bundle/pydevd_trace_dispatch_regular.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(py_db, frame, event, arg)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreadingCurrentThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Undirected graph analysis\n",
    "\n",
    "Construct undirected graph\n",
    "\"\"\"\n",
    "\n",
    "# convert to undirected so we can apply undirected algorithms\n",
    "undirected_graph = graph.to_undirected()\n",
    "print(nx.info(undirected_graph))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "209375\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Undirected graph analysis\n",
    "\n",
    "Check for connected components\n",
    "\"\"\"\n",
    "\n",
    "from networkx.algorithms.components import connected_components\n",
    "\n",
    "# check for connected components. may be a way to prune subgraphs that do not relate to English etymology\n",
    "conn_components = list(connected_components(undirected_graph))\n",
    "connected_component_count = len(conn_components)\n",
    "print(connected_component_count)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "# grab the nodes that have the eng tag, then build the connected graph from those\n",
    "english_nodes = [n for n in graph.nodes() if n[0:3] == 'eng']\n",
    "nodes_to_add = []\n",
    "# add nodes inside loop to avoid having to flatten later\n",
    "[nodes_to_add.extend(bfs_tree(graph, source=node)) for node in english_nodes]\n",
    "\n",
    "english_graph = graph.subgraph(nodes_to_add)\n",
    "print(nx.info(english_graph))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-120a45c5006e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnodes_by_degree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_by_degree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'english_graph' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'english_graph' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "nodes_by_degree = sorted(english_graph.degree, key=lambda x: x[1], reverse=True)\n",
    "print(nodes_by_degree[0:99])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}